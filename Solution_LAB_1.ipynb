{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution to the lab 1\n",
    "In this notebook, we use the following modules `numpy` and `maze`. The latter is a home made module, where all the solutions to the questions are implemented. We will refer to it at each answer, and we encourage you to read it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import maze_lab1 as mz \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Shortest path in the maze\n",
    "\n",
    "The objective of problem 1 is to solve the shortest path problem in a maze. We start first by describing the maze as a numpy array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDP formulation\n",
    "\n",
    "We propose the following MDP formulation: \n",
    "\n",
    "#### State space $\\mathcal{S}$\n",
    "We model the state space separately for the human player and the minotaur in the maze. We have excluded the positions where the walls are located from the possible states of the human player. However, since the minotaur is capable of going through the wall, we defined the minotaur states as all possible positions in the maze. Formally, the state space is\n",
    "\n",
    "$$\\mathcal{S_{player}} = \\big\\lbrace (x,y):\\textrm{such that the cell\n",
    "} (x,y) \\textrm{ is not an obstacle}\\big\\rbrace$$\n",
    "\n",
    "$$\\mathcal{S_{minotaur}} = \\big\\lbrace (x,y)\\big\\rbrace$$\n",
    "\n",
    "\n",
    "#### Action space $\\mathcal{A}$\n",
    "We allow the player to chose to either move `left`, `right`, `down`, `up` or not move at all (`stay`). Note that sometimes the player cannot move in a certain direction because of an obstacle or a wall, yet we permit this to be action. We will see that this is not an issue as long as we define our transition probabilities and rewards appropriately. We also define the action space of minotaur similarly. However, we do permit the move through the wall in this case. \n",
    "Formally, the action space is\n",
    "\n",
    "$$\\mathcal{A_{player}} = \\lbrace \\textrm{up}, \\textrm{ down}, \\textrm{ left}, \\textrm{ right}, \\textrm{ stay} \\rbrace$$\n",
    "\n",
    "$$\\mathcal{A_{minotaur}} = \\lbrace \\textrm{up}, \\textrm{ down}, \\textrm{ left}, \\textrm{ right}, \\textrm{ stay} \\rbrace$$\n",
    "\n",
    "\n",
    "#### Transition probabilities $\\mathcal{P}$\n",
    "Note that there is no randomness involved upon taking an action by the player. As a consequence, the transition probabilities are deterministic. More precisely,   \n",
    "- If at state (or position) $s$ taking action (or move) $a$ does not lead to a wall or an obstacle but to another state (or position) $s'$, then $\\mathbb{P}(s' \\vert s, a) = 1$. \n",
    "- If at state (or position)  $s$ taking action (or move) $a$ leads to a wall or an obstacle, the player remains in his state (or position) $s$, then $\\mathbb{P}(s \\vert s, a) = 1$.\n",
    "- If the state (or position) $s_{player}$ is the same as $s_{minotaur}$, then the player will be caught by the minotaur, $\\mathbb{P}(s_{player} \\vert s_{player} = s_{minotaur}, a) = 0$.\n",
    "\n",
    "\n",
    "#### Rewards $\\mathcal{R}$\n",
    "The objective of the player is to find the exit of the maze while avoiding the obstacles.\n",
    "   - If at state $s$, taking action $a$, leads to be caught by the minotaur, then $r(s,a) = -200$\n",
    "   - If at state $s$, taking action $a$, leads to a wall or an obstacle then $r(s,a) = -100$\n",
    "   - If at state $s$, taking action $a$, leads to some other position in the maze that is not the exit nor a wall nor an obstacle, then $r(s, a) = -1$. \n",
    "   - If at state $s$, taking action $a$, leads to the exit then $r(s ,a) = 0$. \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = np.array([\n",
    "    [0, 0, 1, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 1, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 1, 1, 1],\n",
    "    [0, 0, 1, 0, 0, 1, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 1, 1, 1, 1, 1, 0],\n",
    "    [0, 0, 0, 0, 1, 2, 0, 0]\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mz.draw_maze(maze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = mz.Maze(maze)\n",
    "env.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Finite horizon\n",
    "horizon = 20\n",
    "# Solve the MDP problem with dynamic programming \n",
    "V, policy= mz.dynamic_programming(env,horizon);\n",
    "\n",
    "method = 'DynProg'\n",
    "start_player = (0,0)\n",
    "start_minotaur = (6, 5)\n",
    "path_player, path_minotaur = env.simulate(start_player, start_minotaur, policy, method)\n",
    "\n",
    "mz.animate_solution(maze, path_player, path_minotaur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_games = 100\n",
    "p = 1.0/30.0\n",
    "\n",
    "for n in range(n_games):\n",
    "    horizon = np.random.geometric(p)\n",
    "    # Solve the MDP problem with dynamic programming \n",
    "    V, policy= mz.dynamic_programming(env,horizon);\n",
    "\n",
    "    method = 'DynProg'\n",
    "    start_player = (0,0)\n",
    "    start_minotaur = (6, 5)\n",
    "    path_player, path_minotaur = env.simulate(start_player, start_minotaur, policy, method, horizon)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
